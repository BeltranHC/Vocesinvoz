# ğŸ§  SignSpeak: Traductor de ASL a Voz en Tiempo Real

Una aplicaciÃ³n web en tiempo real que utiliza tu cÃ¡mara web y un modelo de aprendizaje automÃ¡tico para reconocer el **alfabeto del Lenguaje de SeÃ±as Americano (ASL)** y convertirlo en **texto y voz**. Construida con **Flask**, **OpenCV**, **MediaPipe** y **gTTS**, con una interfaz **HTML/CSS/JS** limpia y responsiva.

---

## âœ¨ CaracterÃ­sticas

- ğŸ“· DetecciÃ³n de manos en tiempo real usando MediaPipe
- ğŸ”  Reconoce seÃ±as estÃ¡ticas de ASL para 26 letras + "espacio", "nada" y "del"
- ğŸ§  Modelo de ML (entrenado con 63 caracterÃ­sticas de puntos de MediaPipe)
- âœï¸ Construye frases completas a partir de las seÃ±as en secuencia
- ğŸ—£ï¸ Convierte la frase construida en voz usando Google Text-to-Speech
- ğŸ¨ Interfaz moderna y responsiva (creada desde cero con HTML y CSS)

---

## ğŸ“‚ Estructura de Carpetas

```
project/
â”œâ”€â”€ app.py                    # Backend principal en Flask
â”œâ”€â”€ requirements.txt          # Dependencias de Python
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ asl_model.joblib      # Modelo de ML (descarga automÃ¡tica)
â”‚   â””â”€â”€ label_encoder.joblib  # Codificador de etiquetas (descarga automÃ¡tica)
â”œâ”€â”€ static/
â”‚   â””â”€â”€ audio/                # Contiene archivos .mp3 generados
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html            # PÃ¡gina web frontend
```

---

## ğŸ§  Â¿CÃ³mo Funciona?

1. OpenCV captura video desde la cÃ¡mara web.
2. MediaPipe extrae 21 puntos de referencia de la mano (x, y, z = 63 caracterÃ­sticas).
3. El modelo RandomForestClassifier predice la letra ASL.
4. Una letra solo se agrega a la frase si:
   - Es estable en varios fotogramas, y
   - La confianza del modelo es **85% o mÃ¡s**.
5. El usuario puede presionar:
   - âœ… **Hablar** â†’ convierte la frase en un archivo de audio
   - âŒ **Limpiar** â†’ reinicia la frase actual
6. El audio se guarda y se reproduce en el navegador.

---

## ğŸš€ Comenzando

### 1. Clona el Repositorio

```bash
git clone https://github.com/BeltranHC/Vocesinvoz.git
cd signspeak
```

### 2. Instala los Requisitos de Python

```bash
pip install -r requirements.txt
```

### 3. Prepara tu Modelo

**OpciÃ³n A: Entrenar tu propio modelo (Recomendado)**

```bash
# Paso 1: Recolectar datos con la cÃ¡mara
python collect_data.py

# Paso 2: Entrenar el modelo
python train_model.py

# Paso 3: Evaluar el modelo (opcional)
python evaluate_model.py
```

**OpciÃ³n B: Usar imÃ¡genes existentes**

1. Crea la estructura de carpetas en `data/`:
```
data/
â”œâ”€â”€ A/
â”œâ”€â”€ B/
â”œâ”€â”€ C/
â””â”€â”€ ...
```

2. Coloca tus imÃ¡genes en las carpetas correspondientes
3. Ejecuta `python train_model.py`

### 4. Ejecuta la AplicaciÃ³n

```bash
python app.py
```

Visita [http://localhost:5000](http://localhost:5000) en tu navegador.

---

## ğŸ§  Detalles del Modelo

El modelo utilizado es un clasificador tradicional entrenado con 63 caracterÃ­sticas extraÃ­das de los 21 puntos de referencia de la mano de MediaPipe (x, y, z por punto). El modelo estÃ¡ entrenado con mÃ¡s de 60,000 ejemplos que abarcan:

- 26 letras (A-Z)
- Espacio
- Nada (mano en reposo / fondo)
- Del (eliminar carÃ¡cter)

### ğŸ¯ PrecisiÃ³n Alcanzada

- PrecisiÃ³n general: **98.47%**
- Macro Promedio F1-Score: **0.98**

El modelo se guarda como `asl_model.joblib` junto con `label_encoder.joblib`. Ambos se descargan automÃ¡ticamente vÃ­a `gdown` en la primera ejecuciÃ³n de la app.

---

## ğŸ“¦ Archivos del Modelo

Los archivos del modelo (`asl_model.joblib`) y el codificador (`label_encoder.joblib`) deben estar en la carpeta `model/`.

### ğŸ”§ Entrenar tu Propio Modelo

Este proyecto incluye herramientas completas para entrenar tu propio modelo:

1. **RecolecciÃ³n de Datos**: `collect_data.py`
   - Interfaz interactiva para capturar imÃ¡genes con la cÃ¡mara
   - DetecciÃ³n automÃ¡tica de manos con MediaPipe
   - OrganizaciÃ³n automÃ¡tica de datos por clases

2. **Entrenamiento**: `train_model.py`
   - ExtracciÃ³n de caracterÃ­sticas con MediaPipe
   - Entrenamiento con RandomForestClassifier
   - GeneraciÃ³n de reportes y mÃ©tricas detalladas

3. **EvaluaciÃ³n**: `evaluate_model.py`
   - EvaluaciÃ³n de modelos existentes
   - ComparaciÃ³n entre diferentes modelos
   - Pruebas en tiempo real con imÃ¡genes especÃ­ficas

### ğŸ“Š Estructura de Datos

```
data/
â”œâ”€â”€ A/
â”‚   â”œâ”€â”€ A_0001.jpg
â”‚   â”œâ”€â”€ A_0002.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ B/
â”œâ”€â”€ C/
â”œâ”€â”€ ...
â”œâ”€â”€ space/
â”œâ”€â”€ del/
â””â”€â”€ nothing/
```

> **RecomendaciÃ³n**: MÃ­nimo 100 imÃ¡genes por clase para obtener buenos resultados.

---

## ğŸ“¦ Requisitos

- `Flask`
- `opencv-python`
- `mediapipe`
- `numpy`
- `scikit-learn`
- `joblib`
- `gtts`
- `matplotlib`
- `seaborn`
- `tqdm`
- `pandas`

Instala con:

```bash
pip install -r requirements.txt
```

---

## ğŸ› ï¸ Mejoras Futuras

- [ ] Agregar predicciÃ³n a nivel de palabra o frase usando 3D CNNs
- [ ] Soportar seÃ±as dinÃ¡micas con LSTM o Transformers
- [ ] Integrar modelo de lenguaje (T5, GPT) para limpiar frases
- [ ] Desplegar como demo pÃºblica en Render o Hugging Face Spaces

---


## ğŸ‘¤ Autor

**Junior Huaraya**  
Construido con â¤ï¸ para unir la comunicaciÃ³n entre el mundo Sordo y oyente.

> Â¡SiÃ©ntete libre de hacer fork, dar estrella â­ y compartirlo con otros!
